=======================
Backend Python Tutorial
=======================

.. _Backend_Tutorial:

Welcome to the backend tutorial on how to programmatically use :ref:`DQMaRC <api.DQMaRC>`. 
We begin by importing the necessary libraries including `pandas <https://pandas.pydata.org/docs/index.html>`_ and the :ref:`DataQuality class <api.DataQuality>`. 
In this case, we will also import some functions from the :ref:`api.UtilitiesDQMaRC` module, 
including :func:`MetricCalculator <UtilitiesDQMaRC.MetricCalculator>`, :func:`BarPlotGenerator <UtilitiesDQMaRC.BarPlotGenerator>`, 
and :func:`DonutChartGenerator <UtilitiesDQMaRC.DonutChartGenerator>`. 
These are supplementary functions for quick visualisation of the data quality markup report. 
The :func:`~UtilitiesDQMaRC.overall_quality_fx` function calculates the overall quality.

Source Data
===============
Import Libraries
----------------

.. jupyter-execute::

    import pandas as pd
    from pkg_resources import resource_filename

    # Import DQMaRC
    from DQMaRC import DataQuality

    # Import UtilitiesDQMaRC for data quality visualisation
    from DQMaRC.UtilitiesDQMaRC import (
        MetricCalculator, 
        BarPlotGenerator, 
        DonutChartGenerator, 
        overall_quality_fx, col_good, col_bad)


#. :ref:`DQMaRC <api.DQMaRC>`: the main package to assess multiple data quality dimensions for a given dataset.

#. :ref:`api.UtilitiesDQMaRC`: a suite of modules to support the initial visualisation of the data quality markup report produced by :ref:`DQMaRC <api.DQMaRC>`. These utilities are also used to generate the shiny frontend interface. 

#. :func:`MetricCalculator <UtilitiesDQMaRC.MetricCalculator>`: calculates sum totals and averages of data quality errors for each dimension. 

#. :func:`BarPlotGenerator <UtilitiesDQMaRC.BarPlotGenerator>`: accepts the summary results produced by :func:`MetricCalculator <UtilitiesDQMaRC.MetricCalculator>` to generate a `plotly <https://plotly.com/python/>`_ barplot for quick visualisation of data quality averages. 

#. :func:`overall_quality_fx <UtilitiesDQMaRC.overall_quality_fx>`: a function to generate an overall average of all data quality metrics and fields. 

#. :func:`col_good <UtilitiesDQMaRC.col_good>` / :func:`col_bad <UtilitiesDQMaRC.col_bad>`: colour encodings for good and poor data quality percentages. 


Data Import
-----------

.. figure:: ../images/mockaroo.png
    :alt: A screenshot image showing the image icon for synthetic data generator website Mockaroo.
    :target: https://mockaroo.com/


We will use a synthetic dataset generated by `Mockaroo <https://mockaroo.com/>`_ and read it in as a `pandas DataFrame <https://pandas.pydata.org/docs/reference/frame.html>`_. 
This data is included in the package but is **purely synthetic**. 

.. jupyter-execute::


    # Read in example data as pandas dataframe
    #df = pd.read_csv('../DQMaRC/data/toydf_subset.csv')
    df = pd.read_csv(resource_filename('DQMaRC', 'data/toydf_subset.csv'))
    df

..
    DataGrid(
        df, 
        editable=False, 
        header_visibility='column',
        base_column_size = 180,
        base_column_header_size = 30,
        layout={"height": "300px"},
        header_renderer=TextRenderer(background_color='lightblue'),
        grid_style={"header_background_color": "lightblue"}
        )
    
.. _backend_tutorial.test_params:

Test Parameters Setup
=====================
Initialise DQMaRC
-----------------

:ref:`DQMaRC <api.DQMaRC>` must be initialised by applying the :ref:`api.DataQuality` class to your source dataset. 
:ref:`DQMaRC <api.DQMaRC>` then requires configuration of the :py:attr:`test_params attribute <DQMaRC.DataQuality.test_params>`.
This is a dataset that :ref:`DQMaRC <api.DQMaRC>` uses to map which test parameters (or data quality tests) should be applied 
to the relevant fields or variables from the source dataset. 
Each test parameter is functionally supported by a respective method provided by :ref:`DQMaRC <api.DQMaRC>`.
For example, the ``Completeness_NULL`` test parameter is calculated by the :py:meth:`test_null() method <DQMaRC.Completeness.test_null>`.

If this is your first time using :ref:`DQMaRC <api.DQMaRC>`, you can automatically generate a :py:attr:`test_params <DQMaRC.DataQuality.test_params>`
template using the :py:meth:`get_param_template() method <DQMaRC.DataQuality.get_param_template>`. 
This template is based off the source dataset and automatically 
activates the :ref:`api.completeness` and :ref:`api.uniqueness` parameters for all source data fields. In other words, users
can run :ref:`DQMaRC <api.DQMaRC>` using the auto-generated template to see what the outputs look like and determine whether
this is the right tool for the job. 

We strongly encourage users to get familiar with the test parameters and to take the time to customise it. 
All test parameters are listed and explained in a :ref:`table below <backend_tutorial.test_params_definitions>`.
We recommend customising them using Excel due to ease of use, but python users also have the option to customise test parameters programmatically.
We demonstrate how to this :ref:`here <backend_tutorial.test_params_configurations>`.

.. jupyter-execute::

    # Initialise a DQ object by passing your data to the tool
    dq = DataQuality(df)

    # Retrieve default test parameter form the object. We will edit this in the next step
    test_params = dq.get_test_params()

    # View the test parameters template
    test_params
..
    DataGrid(
        test_params, 
        editable=True, 
        header_visibility='column',
        base_column_size = 180,
        base_column_header_size = 30,
        layout={"height": "300px"},
        header_renderer=TextRenderer(background_color='#fb9a99'),
        grid_style={"header_background_color": "#fb9a99"}
        )

.. _backend_tutorial.test_params_configurations:

Test Parameters Configurations
------------------------------

Now we will demonstrate how to edit the ``test parameters`` programmatically.
You can also export the ``test parameters`` template as a csv file and edit them in MS Excel.


Datetime Format
^^^^^^^^^^^^^^^

The first test parameter to edit is the ``Date_Format``. Although this is not strictly speaking a data quality metric, 
it does help python to correctly identify and calculate other data quality metrics for datetime fields.

.. jupyter-execute::

    # Datetime format
    test_params.loc[test_params['Field']=='Date_of_Diagnosis', 'Date_Format'] = "%d/%m/%Y"
    test_params.loc[test_params['Field']=='Date_of_Birth', 'Date_Format'] = "%d/%m/%Y"
    test_params.loc[test_params['Field']=='Datetime_Event1', 'Date_Format'] = "%d/%m/%Y %H:%M"
    test_params.loc[test_params['Field']=='Datetime_Logging1', 'Date_Format'] = "%d/%m/%Y %H:%M"

    # Another way to do this: 
    # test_params.at[3,'Date_Format']="%d/%m/%Y" # Date of Diagnosis
    # test_params.at[4,'Date_Format']="%d/%m/%Y" # Date of Birth
    # test_params.at[15,'Date_Format']="%d/%m/%Y %H:%M" # Datetime_Event1
    # test_params.at[16,'Date_Format']="%d/%m/%Y %H:%M" # Datetime_Logging1


Completeness Parameters
^^^^^^^^^^^^^^^^^^^^^^^
The first data quality dimension we will edit is :ref:`api.completeness`. When you initialise the 
:py:attr:`test parameters <DQMaRC.DataQuality.test_params>` template, :ref:`DQMaRC <api.DQMaRC>` assumes that the 
user may wish to calculate the number of NULL or Empty records for all fields. As such, the default values for 
``Completeness_NULL`` (see the :py:meth:`test_null() method <DQMaRC.Completeness.test_null>`) and 
``Completeness_Empty`` (see the :py:meth:`test_empty() method <DQMaRC.Completeness.test_empty>`) are ``TRUE``. 
We encourage the user to think of or look for other values that may signify missing data. 

For example, values such as *"Unknown"*, *"Not Known"*, or *"na"* may indicate data missingness. To do this, we set the test parameter
``Completeness_Encoded`` == ``TRUE`` (see the :py:meth:`test_na_strings() method <DQMaRC.Completeness.test_na_strings>`) 
for the field in question (in this case it is "Gender"). Then we include the relevant missing codes in the ``Completeness_Encoded_Mapping``.
Each unique possible value or string encoding for missing data is separated by a pipe, e.g. 'Unknown|Not Known|na' (i.e. Unknown or Not Known or na)

.. jupyter-execute::

    # Completeness
    # Set to true for all fields
    test_params['Completeness_NULL'] = True # Default value
    test_params['Completeness_Empty'] = True # Default value

    # Set Completeness Encoding only for the "Gender" column
    test_params.at[2,'Completeness_Encoded']=True
    test_params.at[2,'Completeness_Encoded_Mapping']='Unknown|Not Known|na' # use pipes for multiple possible codes for missingness


Uniqueness Parameters
^^^^^^^^^^^^^^^^^^^^^
:ref:`api.uniqueness` evaluates if records are unique across the fields that are set to ``TRUE``. In most cases, we assume that records should be unique
across at least all fields combined. 
Therefore, :ref:`DQMaRC <api.DQMaRC>` by default sets the parameter ``Uniqueness_rows`` == ``TRUE`` for all fields. 
However, if records are expected to be unique across select fields, then we encourage the user to set ``TRUE`` 
only to those respective fields, and ``FALSE`` to the remaining fields. 

.. jupyter-execute::

    # Uniqueness
    # Set to true for all fields
    test_params['Uniqueness_rows'] = True # Default value


Consistency Parameters
^^^^^^^^^^^^^^^^^^^^^^^
:ref:`api.consistency` is the evaluation of whether data between two fields in the same dataset are consistent as expected. It consists of two key metrics, 
including ``Consistency_Compare`` (see the :py:meth:`test_one_to_one() method <DQMaRC.Consistency.test_one_to_one>`), 
which is the comparison of values between two fields, and 
``Consistency_Date_Relations`` (see the :py:meth:`date_relationships() method <DQMaRC.Consistency.date_relationships>`), which is a check of 
consistency in logical relationships between two datetime fields. 

We demonstrate the first consistency metric by comparing ``Metastatic_Indicator`` with ``Tumour_M_Stage``. 
In theory, both variables indicate whether cancer has spread to distant locations, but in real-world it is possible that
these may come from different sources. The values in both fields are recorded differently, but mean similar things.
For example, if ``Metastatic_Indicator`` == ``Absent``, then we expect that ``Tumour_M_Stage`` == ``M0``, meaning no metastasis is present.
If ``Metastantic_Indicator`` == ``Present``, then we expect to see ``Tumour_M_Stage`` with either ``M1``, ``M1a``, or other values, but here
we abbreviate it to keep things simple. 

For datetime fields, we may expect one date to occur before or after another in the same dataset. In this example, we demonstrate this using
``Date_of_Birth`` which obviously should occur before ``Date_of_Death``. Importantly, we tell :ref:`DQMaRC <api.DQMaRC>` to raise an error if it detects that: 
``Date_of_Birth`` > ``Date_of_Death``.

.. jupyter-execute::

    # Consistency
    # Set up consistency checks between the "Metastatic_Indicator" and "Tumour_M_Stage" columns
    test_params.loc[test_params['Field']=='Metastatic_Indicator', 'Consistency_Compare'] = True
    test_params.loc[test_params['Field']=='Metastatic_Indicator', 'Consistency_Compare_Field'] = 'Tumour_M_Stage'
    test_params.loc[test_params['Field']=='Metastatic_Indicator', 'Consistency_Compare_Mapping'] = '{["Absent"]: ["M0"], ["Present"]: ["M1", "M1a"]}'

    # Set up consistency checks between date fields "Date_of_Birth" and "Date_of_Diagnosis"
    test_params.loc[test_params['Field']=='Date_of_Birth', 'Consistency_Date_Relations'] = True
    test_params.loc[test_params['Field']=='Date_of_Birth', 'Consistency_Date_Relations_Field'] = 'Date_of_Diagnosis'
    test_params.loc[test_params['Field']=='Date_of_Birth', 'Consistency_Date_Relationship'] = '>' # i.e. raise an error if Date of Birth > Date of Diagnosis


Timeliness Parameters
^^^^^^^^^^^^^^^^^^^^^^^

:ref:`api.timeliness` describes how fresh or up-to-date data is. 
One way to measure this is to define a threshold in time difference (see the :py:meth:`date_diff_calc() method <DQMaRC.Timeliness.date_diff_calc>`) between fields that record 
the timing of events and when those events are captured into the system. For example, in this case, the ``Datetime_Event1`` field signifies the time
that a user says an event occured, or when that data was observed in the real-world, whereas ``Datetime_Logging1`` is the electronically generated datetime
log data indicating when the user entered this data into the system. In a clinical or health setting, the freshness of data may be crucial.

For example, in critical care, patient observation data may be recorded several minutes or hours after the time of observation, instead of immediately. 
To flag these errors, we tell :ref:`DQMaRC <api.DQMaRC>` to raise an error if the time difference between ``Datetime_Event1`` and ``Datetime_Logging1`` is greater than
10 minutes. 

.. jupyter-execute::

    # Timeliness
    # Raise an error if a time difference threshold of 10 minutes is exceeded
    test_params.loc[test_params['Field']=='Datetime_Event1', 'Timeliness_Date_Diff'] = True
    test_params.loc[test_params['Field']=='Datetime_Event1', 'Timeliness_Date_Diff_Field'] = 'Datetime_Logging1'
    test_params.loc[test_params['Field']=='Datetime_Event1', 'Timeliness_Date_Diff_Threshold'] = '10' # i.e. raise an error if timediff >10 minutes


Validity Parameters
^^^^^^^^^^^^^^^^^^^^^^^

Data :ref:`api.validity` describes data that conform to the expected standards, patterns, or ranges that reflect the intended real-world objects. 
:ref:`DQMaRC <api.DQMaRC>` has 7 validity metrics that are shown below. These include: 

#. **Future Dates**: flag a record containing a date that occur in the future (see :py:meth:`Validity_Dates_Future() method <DQMaRC.Validity.test_future_dates>`).

    .. jupyter-execute::

        # (1) Future Dates
        test_params.loc[test_params['Field']=='Date_of_Diagnosis', 'Validity_Dates_Future'] = True
        test_params.loc[test_params['Field']=='Date_of_Birth', 'Validity_Dates_Future'] = True


#. **Date Outliers**: checks if date records fall outside expected date ranges (see :py:meth:`min_max_dates() method <DQMaRC.Validity.min_max_dates>`).

    .. jupyter-execute::

        # (2) Date Outliers
        test_params.loc[test_params['Field']=='Date_of_Diagnosis', 'Validity_Date_Range'] = True
        test_params.loc[test_params['Field']=='Date_of_Birth', 'Validity_Date_Range'] = True

        test_params.loc[test_params['Field']=='Date_of_Diagnosis', 'Validity_Date_Range_Min'] = '2011-01-01'
        test_params.loc[test_params['Field']=='Date_of_Birth', 'Validity_Date_Range_Min'] = '1900-01-01'

        test_params.loc[test_params['Field']=='Date_of_Diagnosis', 'Validity_Date_Range_Max'] = '2023-07-07'
        test_params.loc[test_params['Field']=='Date_of_Birth', 'Validity_Date_Range_Max'] = '2023-01-01'


#. **Numerical Outliers**: checks if numerical data fall outside expected numerical ranges (see :py:meth:`test_ranges() method <DQMaRC.Validity.test_ranges>`).

    .. jupyter-execute::

        # (3) Numerical Outliers 
        test_params.loc[test_params['Field']=='Age', 'Validity_Range'] = True
        test_params.loc[test_params['Field']=='Age', 'Validity_Range_Min'] = 0
        test_params.loc[test_params['Field']=='Age', 'Validity_Range_Max'] = 120

        test_params.loc[test_params['Field']=='Height_cm', 'Validity_Range'] = True
        test_params.loc[test_params['Field']=='Height_cm', 'Validity_Range_Min'] = 20
        test_params.loc[test_params['Field']=='Height_cm', 'Validity_Range_Max'] = 210


#. **NHS Number Validator**: checks if NHS numbers are valid (see :py:meth:`validate_nhs_number() method <DQMaRC.Validity.validate_nhs_number>`).

    .. jupyter-execute::

        # (4) NHS Number Validator
        # test_params.loc[test_params['Field']=='NHS_Number', 'Validity_NHS_Number'] = True


#. **UK Postcode Validator**: checks if UK postcodes are valid (see :py:meth:`test_postcode() method <DQMaRC.Validity.test_postcode>`).

    .. jupyter-execute::

        # (5) UK Postcode Validator
        test_params.loc[test_params['Field']=='Postcode', 'Validity_Postcode_UK'] = True


#. **Data Standards**: checks if values conform to expected permissible values as defined by a data standard (see :py:meth:`test_against_lookup_tables() method <DQMaRC.Validity.test_against_lookup_tables>`).

    #. Either create and save your own data standard

        .. jupyter-execute::

            # (6a) Data Standards
            test_params.loc[test_params['Field']=='Tumour_M_Stage', 'Validity_Lookup'] = True
            test_params.loc[test_params['Field']=='Tumour_M_Stage', 'Validity_Lookup_Type'] = 'Values'
            test_params.loc[test_params['Field']=='Tumour_M_Stage', 'Validity_Lookup_Codes'] = 'M0|M1|M1b|pM1'


    #. Or access a pre-defined list saved as a csv file
    
        .. jupyter-execute::

            #(6b) Use an external csv data standard list of valid codes
            lu_filename = '..DQMaRC/data/lookups/LU_toydf_gender.csv'

            # Here we will apply a pre-defined data standard for "gender"
            test_params.loc[test_params['Field']=='Gender', 'Validity_Lookup'] = True
            test_params.loc[test_params['Field']=='Gender', 'Validity_Lookup_Type'] = 'File'
            test_params.loc[test_params['Field']=='Gender', 'Validity_Lookup_Codes'] = lu_filename


#. **Regular Expression Pattern**: checks if data conform to expected pattern as defined by regular expression (see :py:meth:`test_pattern_validity() method <DQMaRC.Validity.test_pattern_validity>`).

    .. jupyter-execute::

        # (7) Regular Expression Pattern
        test_params.loc[test_params['Field']=='Datetime_Event1', 'Validity_Pattern'] = True
        test_params.loc[test_params['Field']=='Datetime_Event1', 'Validity_Pattern_Regex'] = "(\d{2})/(\d{2})/(\d{4}) (\d{2}):(\d{2})"

        test_params.loc[test_params['Field']=='Datetime_Event2', 'Validity_Pattern'] = True
        test_params.loc[test_params['Field']=='Datetime_Event2', 'Validity_Pattern_Regex'] = "[0-9]{2}/[0-9]{2}/[0-9]{4} [0-9]{2}:[0-9]{2}"


Accuracy Parameters
^^^^^^^^^^^^^^^^^^^^^^^

:ref:`api.accuracy` measures the consistency of input data compared to a known ``gold standard``. In this case, we will set the gold standard
as the input data itself to demonstrate the functionality. 

.. jupyter-execute::

    # Accuracy
    # Set a manually validated version of the data set as the gold standard
    test_params['Gold_Standard'] = True

    # supply gold stand data - we are using the same dataset here for ease
    dq.accuracy.set_gold_standard(df)


.. _backend_tutorial.test_params_definitions:

Test Parameters Definitions
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Here is a list of all ``test parameters`` and their respective descriptions and permissible configurations. 

.. jupyter-execute::

    # Read in example data as pandas dataframe
    test_params_definitions = pd.read_csv('../DQMaRC/data/test_params_definitions.csv')
    test_params_definitions
..
    DataGrid(
        test_params_definitions, 
        editable=False, 
        header_visibility='column',
        base_column_size = 180,
        base_column_header_size = 30,
        layout={"height": "300px"},
        header_renderer=TextRenderer(background_color='#b2df8a'),
        grid_style={"header_background_color": "#b2df8a"}  
        )


Upload Custom Test Parameters
-----------------------------
We have shown how you can programmatically edit the ``test parameters`` dataframe. However, users may opt to make these edits
in Excel instead. In fact, we encourage users to spend the time to do this to maximise the relevance of the data quality output reports.
Here we show how you can upload a pre-defined test parameters=. 

.. jupyter-execute::

    test_params_upload = pd.read_csv(resource_filename('DQMaRC', 'data/toydf_subset_test_params_24.05.16.csv'))
    test_params_upload

..
    DataGrid(
        test_params_upload, 
        editable=False, 
        header_visibility='column',
        base_column_size = 180,
        base_column_header_size = 30,
        layout={"height": "300px"},
        header_renderer=TextRenderer(background_color='#8dd3c7'),
        grid_style={"header_background_color": "#8dd3c7"}  
        )


.. _backend_tutorial.run_DQMaRC:

Set Test Parameters
-------------------
Now we are ready to set the test parameters. We are using the pre-defined uploaded ``test parameters``. 

.. jupyter-execute::

    dq.set_test_params(test_params_upload)


Run DQMaRC
----------

Once you have set your chosen ``test parameters``, you run :ref:`DQMaRC <api.DQMaRC>` by using the 
:py:meth:`run_all_metrics() <DQMaRC.DataQuality.run_all_metrics>` method. Users can optionally run each
data quality dimension separately as indicated in the commented code below.

.. jupyter-execute::
    :hide-output:

    dq.run_all_metrics()

    # To run separately use following methods.
    # dq.completeness.run_metrics()
    # dq.uniqueness.run_metrics()
    # dq.consistency.run_metrics()
    # dq.timeliness.run_metrics()
    # dq.validity.run_metrics()
    # dq.accuracy.run_metrics()


Get DQ Results
==============

Now you can export your results. The two main DQ reports generated by :ref:`DQMaRC <api.DQMaRC>` include the
full cell-level markup and the field-wise aggregated results. 

Full Results
------------
The deepest, and potentially most useful output results is the data quality markup called :py:meth:`DQMaRC.DataQuality.raw_results`. 
This includes a cell-level binary markup of data quality errors, or flags. 
You can join the output dataset from :py:meth:`DQMaRC.DataQuality.raw_results` to the original source dataset by the index.

.. jupyter-execute::

    raw = dq.raw_results()
    raw

    # The full results can be joined to the source data by the index.
    # source_df_raw = df.join(raw)

..
    DataGrid(
        raw, 
        editable=False, 
        header_visibility='column',
        base_column_size = 180,
        base_column_header_size = 30,
        layout={"height": "300px"},
        header_renderer=TextRenderer(background_color='#fb8072'),
        grid_style={"header_background_color": "#fb8072"}  
        )


Aggregated Results
------------------
You can also access a higher-level aggregation of the raw results. 
This contains the sum count of data quality errors detected for each test parameter and each source data field. 
To access this, you can simply use :py:meth:`DQMaRC.DataQuality.aggregate_results` method.

.. jupyter-execute::

    agg = dq.aggregate_results()
    agg
..
    DataGrid(
        agg, 
        editable=False, 
        header_visibility='column',
        base_column_size = 180,
        base_column_header_size = 30,
        layout={"height": "300px"},
        header_renderer=TextRenderer(background_color='#fdbf6f'),
        grid_style={"header_background_color": "#fdbf6f"}  
        )



Data Quality Visualisation
--------------------------
Here we demonstrate examples of data visualisations to gain quick high-level overviews of the DQ results. 

Overall data quality 
^^^^^^^^^^^^^^^^^^^^
The overall quality is the average data quality for all metrics across all fields from the source dataset. 
The applied logic is:
* if average > 90, return "Outstanding"
* if average >= 80, return "Good"
* if average >= 60, return "Requires Improvement"
* if average <60, return "Inadequate"


.. jupyter-execute::


    # Prepare DQ Dashboard
    raw_subset = raw.filter(regex='completeness|validity|consistency|uniqueness_count|accuracy|timeliness')
    calculator = MetricCalculator(raw_subset)
    
    # Simulate the calculation step, calculate aggregates
    calculator.calculate_metrics()
    
    summary_results = calculator.result
    summary_results['Colour_Good'] = summary_results.apply(col_good, axis=1)
    summary_results['Colour_Bad'] = summary_results.apply(col_bad, axis=1)
    summary_results['Colour_NA'] = '#B2C3C6'

    # Overall quality label
    from IPython.display import HTML

    # Function to display overall quality in a Jupyter Notebook
    def display_overall_quality_label():
        if not summary_results.empty:
            data1 = summary_results[summary_results['Prop_NA'] == 0]
            avg_prop_good = data1['Prop_Good'].mean()
            overall_quality_level, background_color, text_colour = overall_quality_fx(avg_prop_good)

            overall_quality_text = f"Overall Quality: {overall_quality_level}"
            html = f"""
            <div style="
                background-color: {background_color};
                padding: 10px;
                border-radius: 5px;
                color: {text_colour};
                border: 2px solid {text_colour};
                text-align: center;
                width: 300px;">
                {overall_quality_text}
            </div>
            """
            return HTML(html)
        else:
            return HTML("<div style='text-align: center;'>No data available</div>")

    # Use the function to display the result
    display_overall_quality_label()


Donut Charts
^^^^^^^^^^^^
The donut charts represent the average DQ for all metrics and fields within a given DQ dimension. 

.. jupyter-execute::

    DonutChartGenerator(summary_results).plot_donut_charts()


Bar Chart Completeness
^^^^^^^^^^^^^^^^^^^^^^
The barcharts represent the average DQ for each field of a given DQ dimension. 
Here we show all relevant fields' average completeness quality.

.. jupyter-execute::

    BarPlotGenerator(summary_results, "completeness").plot_bar()


Bar Chart Consistency
^^^^^^^^^^^^^^^^^^^^^
Here we show all relevant fields' average consistency.

.. jupyter-execute::

    BarPlotGenerator(summary_results, "consistency").plot_bar()


Bar Chart Timeliness
^^^^^^^^^^^^^^^^^^^^
Here we show all relevant fields' average timeliness.

.. jupyter-execute::

    BarPlotGenerator(summary_results, "timeliness").plot_bar()


Bar Chart Uniqueness
^^^^^^^^^^^^^^^^^^^^

Uniqueness indicates if records are unique across the combination of fields chosen. 

.. jupyter-execute::

    BarPlotGenerator(summary_results, "uniqueness").plot_bar()


Bar Chart Validity
^^^^^^^^^^^^^^^^^^

Here we show all relevant fields' average validity.

.. jupyter-execute::

    BarPlotGenerator(summary_results, "validity").plot_bar()


Example Errors by DQ Dimension
------------------------------

First we must join the source data with the data quality markup called raw. We can do this
using a pandas join method. Now, the ``df_DQ_full`` dataframe contains both source data and the 
data quality markup. 

.. jupyter-execute::
    
    # Join source data to the full DQ markup results
    df_DQ_full = df.join(raw, how="left")


Completeness Examples
^^^^^^^^^^^^^^^^^^^^^

In this example, we showcase completeness errors present in the ``Gender`` variable of the source dataset.
Values such as ``.``, ``Na``, ``unknown`` are flagged as errors, which are represented as ``1`` in the adjacent fields
such as ``Completeness_NULL_|_Gender``, ``Completeness_Empty_|_Gender`` and ``Completeness_NULL_|_Encoded``.
The ``completeness_count_|_Gender`` variable is the sum of these three DQ metrics for each variable (i.e. Gender in this case).

.. jupyter-execute::

    gender_completeness_conditions = (df_DQ_full['Completeness_NULL_|_Gender']>0) | \
        (df_DQ_full['Completeness_Empty_|_Gender']>0) | \
            (df_DQ_full['Completeness_Encoded_|_Gender']>0)

    df_DQ_full[['Gender','Completeness_NULL_|_Gender', 
                'Completeness_Empty_|_Gender', 'Completeness_Encoded_|_Gender', 
                'completeness_count_|_Gender']].loc[(gender_completeness_conditions)]
..
    DataGrid(df_DQ_full[['Gender','Completeness_NULL_|_Gender', 
                'Completeness_Empty_|_Gender', 'Completeness_Encoded_|_Gender', 
                'completeness_count_|_Gender']].loc[(gender_completeness_conditions)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "300px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})


Uniqueness Examples
^^^^^^^^^^^^^^^^^^^

Uniqueness evaluates duplicate records for variables where we expect them to be unique. 
Here you can see that the ``row_uniqueness_|_full_row_uniqueness`` variable indicates ``1`` if a duplicate record is present 
across the ``Patient_ID`` and ``Gender`` source variables. 

.. jupyter-execute::

    # Check which rows have duplication where they should be unique
    df_DQ_full[['Patient_ID', 'Gender']][df_DQ_full[['Patient_ID', 'Gender']].duplicated()]

    # Show how uniqueness flags are shown
    uniqueness_conditions = (df_DQ_full['Patient_ID']==1) | (df_DQ_full['Patient_ID']==10)

    df_DQ_full[['Patient_ID', 'Gender', 
                'row_uniqueness_|_full_row_uniqueness']].loc[(uniqueness_conditions)]

..
    DataGrid(df_DQ_full[['Patient_ID', 'Gender', 
                'row_uniqueness_|_full_row_uniqueness']].loc[(uniqueness_conditions)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "150px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})


Consistency Examples
^^^^^^^^^^^^^^^^^^^^

We demonstrate consistency errors below by comparing source variables ``Metastantic_Indicator`` and ``Tumour_M_Stage``.
In the full DQ report, the variable ``Consistency_Compare_|_Metastantic_Indicator`` contains ``1`` if an inconsistency is 
detected between ``Metastantic_Indicator`` and ``Tumour_M_Stage`` based on the mapping provided in the ``test_params`` data.

.. jupyter-execute::

    consistency_conditions_metastatic_indicator = (df_DQ_full['Consistency_Compare_|_Metastatic_Indicator']>0)

    df_DQ_full[['Tumour_M_Stage','Metastatic_Indicator', 
                'Consistency_Compare_|_Metastatic_Indicator', 
                'consistency_count_|_Metastatic_Indicator']].loc[(consistency_conditions_metastatic_indicator)]
..
    DataGrid(df_DQ_full[['Tumour_M_Stage','Metastatic_Indicator', 
                'Consistency_Compare_|_Metastatic_Indicator', 
                'consistency_count_|_Metastatic_Indicator']].loc[(consistency_conditions_metastatic_indicator)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "300px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})



We also demonstrate a consistency check between ``date of birth`` and ``date of diagnosis``.

.. jupyter-execute::

    consistency_conditions_dates = (df_DQ_full['Consistency_Date_Relations_|_Date_of_Diagnosis']>0) | \
        (df_DQ_full['Consistency_Date_Relations_|_Date_of_Birth']>0)

    df_DQ_full[['Date_of_Birth','Date_of_Diagnosis',
                'Consistency_Date_Relations_|_Date_of_Birth',
                'Consistency_Date_Relations_|_Date_of_Diagnosis']].loc[(consistency_conditions_dates)]
..
    DataGrid(df_DQ_full[['Date_of_Birth','Date_of_Diagnosis',
                'Consistency_Date_Relations_|_Date_of_Birth',
                'Consistency_Date_Relations_|_Date_of_Diagnosis']].loc[(consistency_conditions_dates)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "150px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})


Timeliness Examples
^^^^^^^^^^^^^^^^^^^

Here we demonstrate example timeliness errors by comparing the datetime difference between ``Datetime_Event2`` and ``Datetime_Logging2``.
According to the customised ``test_parameters``, we set a threshold of 10 minutes to flag an error if the datetime difference is exceeded.
In otherwords, if the difference in time between these variables is greater than 10 minutes, an error, i.e. ``1``, is present in the 
``Timeliness_Date_Diff_|_Datetime_Event2`` field of the DQ report. 

.. jupyter-execute::

    timeliness_conditions = (df_DQ_full['Timeliness_Date_Diff_|_Datetime_Event2']>0)
    df_DQ_full[['Datetime_Event2','Datetime_Logging2',
                'Timeliness_Date_Diff_|_Datetime_Event2',
                'timeliness_count_|_Datetime_Event2']].loc[(timeliness_conditions)]
..
    DataGrid(df_DQ_full[['Datetime_Event2','Datetime_Logging2',
                'Timeliness_Date_Diff_|_Datetime_Event2',
                'timeliness_count_|_Datetime_Event2']].loc[(timeliness_conditions)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "200px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})


Validity Examples
^^^^^^^^^^^^^^^^^

Here we demonstrate example errors of data validity across all 7 validity metrics, including: future dates, outlier dates, invalid NHS numbers,
invalid codes, numerical outlires, and invalid patterns. 

Future Dates
""""""""""""

Future dates represent if a datetime field occurs in the future relative to the current datetime. This cannot detect a future date if it occurred
in the past. Instead, this may be detected as an outlier date or inconsistent date instead.

.. jupyter-execute::

    validity_future_dates_conditions = (df_DQ_full['Validity_Dates_Future_|_Date_of_Diagnosis']>0)
    df_DQ_full[['Date_of_Diagnosis','Validity_Dates_Future_|_Date_of_Diagnosis']].loc[(validity_future_dates_conditions)]

..
    DataGrid(df_DQ_full[['Date_of_Diagnosis','Validity_Dates_Future_|_Date_of_Diagnosis']].loc[(validity_future_dates_conditions)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "150px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})


Outlier Dates
"""""""""""""

Outlier dates are dates that occur outside the ranges as set in the ``test parameters``.

.. jupyter-execute::

    validity_outlier_dates_conditions = (df_DQ_full['Validity_Date_Range_|_Date_of_Diagnosis']>0)
    df_DQ_full[['Date_of_Diagnosis','Validity_Date_Range_|_Date_of_Diagnosis']].loc[(validity_outlier_dates_conditions)]
..
    DataGrid(df_DQ_full[['Date_of_Diagnosis','Validity_Date_Range_|_Date_of_Diagnosis']].loc[(validity_outlier_dates_conditions)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "300px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})


Invalid NHS Numbers
"""""""""""""""""""
Invalid NHS numbers are flagged when NHS numbers do not meet the requirements as set by the NHS validation algorithm.
Here is how you would check in your dataset which NHS numbers were invalid. We excluded this from our example to 
minimise the risk of sharing potentially true NHS numbers even when synthetically generated.

.. jupyter-execute::

    #validity_NHS_number_conditions = (df_DQ_full['Validity_NHS_Number_|_NHS_number']>0)
    #df_DQ_full[['NHS_number','Validity_NHS_Number_|_NHS_number']].loc[(validity_NHS_number_conditions)]
..
    DataGrid(df_DQ_full[['NHS_number','Validity_NHS_Number_|_NHS_number']].loc[(validity_NHS_number_conditions)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "200px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})


Invalid UK Postcodes
""""""""""""""""""""
Invalid UK postcodes can be detected by the UK postcode validation algorithm.
Other countries' postcodes can be validated using pattern validation as shown below.

.. jupyter-execute::

    validity_UK_postcodes_conditions = (df_DQ_full['Validity_Postcode_UK_|_Postcode']>0)
    df_DQ_full[['Postcode','Validity_Postcode_UK_|_Postcode']].loc[(validity_UK_postcodes_conditions)]
..
    DataGrid(df_DQ_full[['Postcode','Validity_Postcode_UK_|_Postcode']].loc[(validity_UK_postcodes_conditions)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "150px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})


Invalid Tumour Stage Codes
""""""""""""""""""""""""""
This is one example of a validation test applied to a categorical or coded source variable. In this case, we validate Tumour_Stage codes by comparing
against a list of valid codes. We must provide the code list as a csv file and the name of it in the 
``test parameters`` dataset.

.. jupyter-execute::

    validity_codes_conditions = (df_DQ_full['Validity_Lookup_Table_|_Tumour_Stage']>0)
    df_DQ_full[['Tumour_Stage','Validity_Lookup_Table_|_Tumour_Stage']].loc[(validity_codes_conditions)]


Numerical Outliers
""""""""""""""""""

Numerical outliers are detected by applying a minimum and maximum numerical range in the ``test parameters``.

.. jupyter-execute::

    validity_numerical_ranges_conditions = (df_DQ_full['Validity_Range_|_Height_cm']>0)
    df_DQ_full[['Height_cm','Validity_Range_|_Height_cm']].loc[(validity_numerical_ranges_conditions)]

..
    DataGrid(df_DQ_full[['Height_cm','Validity_Range_|_Height_cm']].loc[(validity_numerical_ranges_conditions)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "300px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})


Invalid Patterns
""""""""""""""""

The pattern validation check uses regular expression pattern. Below, you can see a datetime variable with invalid date formats or patterns. 
The ``test paramaters`` are set to only accept patterns that follow ``DD/MM/YYY HH:MM``.

.. jupyter-execute::

    validity_pattern_conditions = (df_DQ_full['Validity_Pattern_|_Datetime_Event1']>0)
    df_DQ_full[['Datetime_Event1','Validity_Pattern_|_Datetime_Event1']].loc[(validity_pattern_conditions)]
..
    DataGrid(df_DQ_full[['Datetime_Event1','Validity_Pattern_|_Datetime_Event1']].loc[(validity_pattern_conditions)],
                header_visibility='column',
                base_column_size = 180,
                base_column_header_size = 30,
                layout={"height": "200px"},
                header_renderer=TextRenderer(background_color='lightblue'),
                grid_style={"header_background_color": "lightblue"})
